
# -*- coding: utf-8 -*-
"""Jalon3_Thomas.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LYTkqc6G6YBtCa3D0L8xgHkN9VjnoiwC

1. Fonction de prediction :

review --> Fonction --> topic

review <br>
--> sentiment analysis (textblob) <br>
--> non --> fin <br>
--> oui --> topic detection (model nmf) --> topic
"""

#!pip install -U textblob
#!pip install contractions
#!pip install streamlit

from textblob import TextBlob

import numpy as np
import pandas as pd

import nltk
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('averaged_perceptron_tagger')
nltk.download('omw-1.4')
from nltk.tokenize import RegexpTokenizer
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import re
import contractions

import sklearn
import streamlit as st
text = '''
The titular threat of The Blob has always struck me as the ultimate movie
monster: an insatiably hungry, amoeba-like mass able to penetrate
virtually any safeguard, capable of--as a doomed doctor chillingly
describes it--"assimilating flesh on contact.
Snide comparisons to gelatin be damned, it's a concept with the most
devastating of potential consequences, not unlike the grey goo scenario
proposed by technological theorists fearful of
artificial intelligence run rampant.
'''

def text2pol(text) :
  blob = TextBlob(text)
  return blob.sentiment.polarity

import pickle #pour save les fichiers en version binaire

nmf_model = pickle.load( open( "model.p", "rb" ) )
vectorizer = pickle.load( open( "vectorizer.p", "rb" ) )

def display_topics(model, feature_names, num_top_words, topic_names=None):
    '''Given an NMF model, feature_names, and number of top words, print topic number and its top feature names, up to specified number of top words.'''
    # iterate through topics in topic-term matrix, 'H' aka
    # model.components_
    for ix, topic in enumerate(model.components_):
        #print topic, topic number, and top words
        if not topic_names or not topic_names[ix]:
            print("\nTopic ", ix)
        else:
            print("\nTopic: '",topic_names[ix],"'")
        print(", ".join([feature_names[i] \
             for i in topic.argsort()[:-num_top_words - 1:-1]]))

def tokenify(my_string) :
  sentence_endings = r"[.,?!]"
  phrase = " ".join(re.split(sentence_endings,my_string))

  spaces = r"\s+"
  token = re.split(spaces, phrase)

  token = [t for t in token if t != '']
  token = [t for t in token if not t.isdigit()]
  return token

def expand(tokens) :
  expanded_words = []
  for word in tokens:
    # using contractions.fix to expand the shotened words
    expanded_words.append(contractions.fix(word))
  expanded_words = " ".join(expanded_words).split()
  expanded_words = [t.lower() for t in expanded_words]
  return expanded_words

def not_management(tokens):
  while 'not' in tokens :
    i = tokens.index('not')
    tokens.remove('not')
    if len(tokens) > i :
      tokens[i] = 'NOT_'+tokens[i]
    else :
      tokens[-1] = 'NOT_'+tokens[-1]
  while 'never' in tokens :
    i = tokens.index('never')
    tokens.remove('never')
    if len(tokens) > i :
      tokens[i] = 'NOT_'+tokens[i]
    else :
      tokens[-1] = 'NOT_'+tokens[-1]
  return tokens

def stop_wrd(tokens, most_freq = None) :
  stop_words = set(stopwords.words('english'))
  filtered_sentence = [w for w in tokens if not w in stop_words]
  if most_freq != None :
    filtered_sentence = [w for w in filtered_sentence if not w in most_freq]
  return filtered_sentence

def lemmatization(tokens) :
  lemmatizer = WordNetLemmatizer()
  token_tag = nltk.pos_tag(tokens)
  lemm_tokens = []
  for word, tag in token_tag :
    if tag.startswith('J'):
      lemm_tokens.append(lemmatizer.lemmatize(word,'a'))
    elif tag.startswith('V'):
      lemm_tokens.append(lemmatizer.lemmatize(word,'v'))
    elif tag.startswith('N'):
      lemm_tokens.append(lemmatizer.lemmatize(word,'n'))
    elif tag == 'PRP' :
      lemm_tokens.append(word)
    elif tag.startswith('R'):
      lemm_tokens.append(lemmatizer.lemmatize(word,'r'))
    else :
      lemm_tokens.append(lemmatizer.lemmatize(word))
  return lemm_tokens

def text_process (sentence, most_freq = None):
  return lemmatization(stop_wrd(not_management(expand(tokenify(sentence))), most_freq))

def text2topic(text, model, feature_names, num_top_words=15, n_topics=1) :
  toks = text_process(text)
  X = vectorizer.transform([" ".join(toks)])
  df = pd.DataFrame(data=X.toarray(), columns=vectorizer.get_feature_names_out())
  W = nmf_model.transform(df)
  test = W[0].argsort()[-n_topics:][::-1]
  ret = []
  for ind in test :
    topics = [topic for topic in model.components_]
    val_lst = topics[ind].argsort()[:-num_top_words - 1:-1]
    words_lst = [feature_names[i] \
        for i in topics[ind].argsort()[:-num_top_words - 1:-1]]
    ret.append([ind, val_lst, words_lst])
  return ret
  """
  for ix, topic in enumerate(model.components_):
    if ix == np.argmax(W, axis=1) :
      print("topic", ix)
      print(", ".join([feature_names[i] \
        for i in topic.argsort()[:-num_top_words - 1:-1]]))
  """

thisdict = {
  "topic0" : "Bad managment",
  "topic1" : "Disappointment",
  "topic2" : "Bad Pizza",
  "topic3" : "Order problem",
  "topic4" : "Bad service",
  "topic5" : "Bad Burger",
  "topic6" : "Too much wait",
  "topic7" : "Bad bar",
  "topic8" : "Bad Chicken",
  "topic9" : "Bad food - Restaurant",
  "topic10": "Bad Sushi",
  "topic11": "Overpriced or underquantitied",
  "topic12": "Bad customer services",
  "topic13": "Bad Sandwich",
  "topic14": "Changes not appreciated"
}

def process_comm(comm, n) :
  # test phrase :
  # it was aweful and i'll never go in this restaurant to buy pizza again ! Also the service was really bad !  And i won't talk about the price !
  num_top_words = 15
  text = text_process(comm)
  if text2pol(comm) <= 0:
    top = text2topic(comm, nmf_model, vectorizer.get_feature_names_out(), num_top_words, n)
    communs = []
    for lst in top:
      commun = [word for word in lst[2] if word in text]
      if commun != [] :
        #st.write(thisdict['topic' + str(lst[0])], lst[1], lst[2])
        communs.append((thisdict['topic' + str(lst[0])], commun))
    # st.write(communs)
    df = pd.DataFrame([" ".join(com[1]) for com in communs], columns=['because of words'], index=[str(com[0]) for com in communs])
    st.table(df)
  else :
    st.write("Wow ! C'est positif !")


st.title('This is the jalon3 !')
st.write("""
# My first app
Hello *world* !
""")


x = st.slider('max value of topics to appear :', min_value=1, max_value=5)  # ðŸ‘ˆ this is a widget

comm = st.text_input("Put your comment in it :", placeholder='Write something here.')
press = st.button('process')
if press :
  process_comm(comm, x)



